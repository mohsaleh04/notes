# فصل چهارم: نتایج و تحلیل داده‌ها

## ۱-۴ مقدمه

این فصل به ارائه و تحلیل نتایج حاصل از پیاده‌سازی و ارزیابی مدل مدیریت هوشمند انرژی مبتنی بر الگوریتم یادگیری تقویتی عمیق DDPG اختصاص دارد. همان‌طور که در فصل سوم تشریح شد، هدف اصلی این مدل، کنترل بهینه و همزمان منابع انرژی کلیدی در یک خانه هوشمند شامل سیستم تهویه مطبوع (HVAC)، سیستم ذخیره‌سازی انرژی (ESS) و پمپ آب است. الگوریتم DDPG به دلیل توانایی در مدیریت فضاهای عمل پیوسته، ابزاری قدرتمند برای یادگیری سیاست‌های کنترلی در چنین محیط‌های پیچیده و پویایی محسوب می‌شود.

فرآیند تحلیل نتایج در این فصل در دو مرحله‌ی مجزا سازماندهی شده است. در مرحله نخست، عملکرد مدل با تنظیمات اولیه و پیش از هرگونه بهینه‌سازی بررسی می‌شود. این تحلیل، نقاط ضعف مدل، ناپایداری‌های فرآیند یادگیری و دلایل عدم دستیابی به سیاست بهینه را آشکار می‌سازد. در مرحله دوم، پس از تشریح فرآیند بهینه‌سازی و تنظیم دقیق هایپرپارامترها، نتایج نسخه نهایی و بهبودیافته مدل ارائه می‌گردد. در این بخش، شاخص‌های کلیدی عملکرد (KPIs) از جمله پایداری یادگیری، هزینه نهایی انرژی، سطح آسایش حرارتی و بهره‌وری منابع انرژی به‌تفصیل تحلیل و با نتایج پژوهش‌های مرجع مقایسه خواهند شد تا اعتبار و کارایی مدل پیشنهادی از منظر علمی و عملی ارزیابی شود.

## ۲-۴ نتایج نسخه اولیه مدل پیش از بهینه‌سازی

در گام نخست، مدل DDPG با مجموعه‌ای از هایپرپارامترهای استاندارد و بدون تنظیمات دقیق، در محیط شبیه‌سازی‌شده آموزش داده شد. هدف از این مرحله، ایجاد یک خط پایه (Baseline) برای ارزیابی و درک چالش‌های ذاتی مدل در مواجهه با مسئله مدیریت انرژی بود.

### ۱-۲-۴ تنظیمات و معماری اولیه مدل

مدل اولیه بر اساس هایپرپارامترهایی که عموماً در مقالات پایه پیشنهاد می‌شوند، پیکربندی شد. این تنظیمات شامل نرخ‌های یادگیری محافظه‌کارانه برای شبکه‌های Actor و Critic، ظرفیت بالای حافظه تجربه (Replay Buffer) و استفاده از نویز گوسی با انحراف معیار ثابت برای اکتشاف بود. معماری شبکه‌های عصبی نیز شامل دو لایه پنهان با توابع فعال‌سازی ReLU و Tanh بود که ساختاری متداول در پیاده‌سازی‌های DDPG است.

(در این بخش می‌توانید «جدول ۴-۱: پارامتر و مقادیر پارامتر اولیه» را قرار دهید.)

### ۲-۲-۴ تحلیل منحنی یادگیری اولیه

برای ارزیابی کیفیت فرآیند یادگیری، نمودارهای پاداش دریافتی عامل در طول زمان تحلیل شدند. نمودار تاریخچه پاداش (Reward History) که پاداش آنی در هر اپیزود را نشان می‌دهد، نوسانات شدید و دوره‌ای را به تصویر می‌کشد. این الگو بیانگر آن است که عامل در یادگیری یک سیاست پایدار ناتوان بوده است. در ابتدای آموزش، پاداش‌ها عمدتاً منفی هستند و اگرچه در مقاطعی بهبود می‌یابند، اما بلافاصله دچار افت شدید شده و یک روند همگرایی مشخص شکل نمی‌گیرد. این بی‌ثباتی از عدم هماهنگی در به‌روزرسانی شبکه‌های Actor و Critic و ضعف در مدیریت تجربیات ذخیره‌شده نشئت می‌گیرد.

(در این بخش می‌توانید «شکل ۴-۱: تغییرات پاداش کل در اپیزودها» را قرار دهید.)

از سوی دیگر، منحنی یادگیری تجمعی (Cumulative Learning Curve) که مجموع پاداش‌ها را نشان می‌دهد، یک روند کلی صعودی دارد. با این حال، شیب این نمودار در ابتدا بسیار کند است و در ادامه نیز به‌صورت غیریکنواخت افزایش می‌یابد تا در نهایت به یک حالت نزدیک به اشباع می‌رسد. این اشباع، نه به دلیل دستیابی به یک سیاست بهینه، بلکه ناشی از نوسانات مداوم و بازیابی‌های مقطعی سیاست توسط عامل است. بنابراین، نمودار تجمعی به‌تنهایی نمی‌تواند ناپایداری ذاتی رفتار عامل را نمایش دهد.

(در این بخش می‌توانید «نمودار ۴-۲: منحنی تجمعی یادگیری عامل» را قرار دهید.)

تحلیل فنی دلایل این عملکرد ضعیف نشان داد که نرخ‌های یادگیری بسیار پایین و نزدیک به هم برای Actor و Critic، به‌روزرسانی کند شبکه‌های هدف (Target Networks) و مهم‌تر از همه، استفاده از یک نویز اکتشافی ثابت، مانع از کاوش مؤثر فضای عمل و یادگیری الگوهای بهینه شده است. در نتیجه، سیاست نهایی برای کنترل منابع انرژی ناپایدار باقی مانده و اهداف اصلی پژوهش محقق نشده است.

### ۳-۲-۴ تحلیل عملکرد منابع انرژی در نسخه اولیه

بررسی رفتار عامل در کنترل هر یک از منابع انرژی، درک عمیق‌تری از نقاط ضعف مدل اولیه ارائه می‌دهد.

**سیستم تهویه مطبوع (HVAC):** به‌طور شگفت‌انگیزی، عملکرد عامل در کنترل دمای داخلی نسبتاً قابل قبول بود. نمودارها نشان می‌دهند که دما تقریباً در تمام مدت در محدوده آسایش (۱۹ تا ۲۴ درجه سانتی‌گراد) و نزدیک به مرز بالایی آن (حدود ۲۳.۹ درجه) باقی مانده است. اگرچه این پایداری در نگاه اول مثبت به نظر می‌رسد، اما عدم وجود نوسان معنادار نشان می‌دهد که عامل احتمالاً یک سیاست ایستا و غیرپویا را اتخاذ کرده و به تغییرات فصلی یا دمای بیرون واکنش بهینه‌ای نشان نداده است.

(در این بخش می‌توانید «نمودار ۴-۳: دمای داخلی (Indoor Temp)» را قرار دهید.)

**سیستم ذخیره‌سازی انرژی (ESS) و پمپ آب:** در مقابل، عملکرد عامل در مدیریت باتری و پمپ آب بسیار ضعیف بود. سطح شارژ باتری (SoC) فاقد هرگونه الگوی منطقی برای شارژ و دشارژ بود و اغلب در نزدیکی مرزهای حداقل باقی می‌ماند. این امر منجر به استفاده ناکارآمد از ظرفیت باتری، افزایش وابستگی به شبکه و استهلاک باتری می‌شد. به طور مشابه، پمپ آب نیز به دفعات بسیار زیاد و بدون یک سیاست مشخص روشن و خاموش می‌شد که نشان‌دهنده عدم درک عامل از تقاضای واقعی آب و منجر به اتلاف انرژی و فرسودگی مکانیکی پمپ می‌گردید.

(در این بخش می‌توانید نمودارهای «۴-۴: سطح ESS»، «۴-۵: انرژی پمپ» را قرار دهید.)

**مصرف انرژی شبکه و خورشیدی:** در نهایت، به دلیل مدیریت ناکارآمد باتری، انرژی خورشیدی تولیدشده نه به‌طور کامل مصرف و نه به‌درستی ذخیره می‌شد. این امر وابستگی به شبکه برق را افزایش داده و هزینه نهایی انرژی را بالا نگه می‌داشت. تحلیل اقتصادی اولیه نشان داد که پتانسیل بالایی برای صرفه‌جویی وجود دارد که در این نسخه از مدل محقق نشده است.

(در این بخش می‌توانید نمودارهای «۴-۶: مصرف شبکه و انرژی خورشیدی» و نمودار هزینه را قرار دهید.)

## ۳-۴ فرآیند بهینه‌سازی و تنظیم مجدد مدل

با توجه به نتایج ناامیدکننده مدل اولیه، فرآیند بهینه‌سازی جامعی با هدف ارتقای پایداری و کارایی عامل یادگیرنده آغاز شد. این فرآیند بر اصلاح هایپرپارامترهای کلیدی، بازنگری در استراتژی اکتشاف و بهبود معماری یادگیری متمرکز بود. تغییرات اعمال‌شده به‌صورت تجربی و با تحلیل مداوم نمودارهای عملکرد صورت گرفت تا بهترین ترکیب برای دستیابی به اهداف پژوهش شناسایی شود.

مهم‌ترین تغییرات اعمال‌شده شامل افزایش متناسب نرخ‌های یادگیری Actor و Critic برای تسریع همگرایی، افزایش نرخ به‌روزرسانی شبکه‌های هدف (τ) برای هماهنگی بهتر، و کاهش ظرفیت حافظه تجربه برای تمرکز بر داده‌های جدیدتر بود. کلیدی‌ترین بهبود، جایگزینی نویز اکتشافی ثابت با یک استراتژی کاهش تدریجی نویز بود. این کار به عامل اجازه داد تا در ابتدای آموزش به کاوش گسترده بپردازد و با پیشرفت یادگیری، به‌تدریج بر بهره‌برداری از سیاست‌های بهینه متمرکز شود. همچنین، افزودن یک دوره گرم کردن (Warm-up) در ابتدای آموزش، کیفیت تجربیات اولیه ذخیره‌شده در حافظه را بهبود بخشید.

(در این بخش می‌توانید «جدول ۴-۷: مقایسه تنظیمات مدل قبل و بعد از بهینه‌سازی» را قرار دهید.)

## ۴-۴ تحلیل نتایج نسخه نهایی مدل (پس از بهینه‌سازی)

اعمال تغییرات و بهینه‌سازی‌های ذکرشده، تأثیر چشمگیری بر عملکرد مدل داشت. در این بخش، نتایج حاصل از نسخه نهایی به‌تفصیل تحلیل می‌شود.

### ۱-۴-۴ تحلیل منحنی یادگیری نسخه نهایی

پس از اعمال بهینه‌سازی‌های هدفمند بر روی ساختار و پارامترهای مدل DDPG، نتایج حاصل از فرآیند آموزش نسخه نهایی نشان‌دهنده بهبود قابل‌توجه در کیفیت سیاست یادگیری‌شده و رفتار عامل در محیط مدیریت انرژی خانه هوشمند است. در این بخش، به تحلیل دقیق و چندلایه از روند پاداش، پراکندگی آماری و تغییر مرزهای پاداش پرداخته می‌شود.

(در این بخش می‌توانید «نمودار ۴-۷: روند همگرایی پاداش در نسخه بهینه‌شده» را قرار دهید.)

تحلیل توزیع آماری پاداش‌ها نیز این پایداری را تأیید می‌کند. همان‌طور که در نمودار توزیع پاداش مشاهده می‌شود، عمده پاداش‌ها در بازه‌ای محدود و نزدیک به مقادیر بهینه (بین ۳۰- تا ۰) متمرکز شده‌اند. این تمرکز، برخلاف پراکندگی شدید در مدل اولیه، نشان می‌دهد که عامل در اکثر مواقع تصمیمات بهینه یا نزدیک به بهینه اتخاذ کرده است.

(در این بخش می‌توانید «نمودار ۴-۸: توزیع فراوانی پاداش‌ها» را قرار دهید.)

علاوه بر این، بررسی روند تغییر مرزهای حداقل و حداکثر پاداش (Rolling Reward Bounds) نشان می‌دهد که نه تنها میانگین پاداش بهبود یافته، بلکه عملکرد عامل در بدترین شرایط نیز به‌مراتب بهتر شده است. این موضوع دلالت بر یادگیری یک سیاست قوی و قابل اتکا دارد که در شرایط مختلف محیطی عملکرد خود را حفظ می‌کند.

(در این بخش می‌توانید «نمودار ۴-۹: روند تغییر مرزهای پاداش» را قرار دهید.)

### ۲-۴-۴ ارزیابی عملکرد منابع انرژی در نسخه نهایی

تحلیل عملکرد هر یک از منابع انرژی در نسخه بهینه‌شده، کارایی سیاست نهایی را بهتر نمایان می‌سازد.

**سیستم تهویه مطبوع (HVAC):** عامل نهایی در کنترل HVAC عملکرد بسیار موفقی داشت. نمودار دمای داخلی نشان می‌دهد که پس از نوسانات اولیه در فاز آموزش، دما به‌طور پایدار در محدوده بهینه (۲۲ تا ۲۴ درجه) تثبیت می‌شود. تحلیل آماری نیز تأیید می‌کند که بیش از ۹۹٪ مواقع، دمای داخلی در ناحیه آسایش قرار داشته و بیشترین فراوانی دما در بازه ۲۲ تا ۲۳.۵ درجه متمرکز بوده است. ارتباط مستقیم بین حفظ دما در این بازه و دریافت پاداش بالا، نشان‌دهنده طراحی موفق تابع پاداش و یادگیری صحیح عامل است.

(در این بخش می‌توانید نمودارهای «۴-۱۰: تغییرات دمای داخلی»، «۴-۱۱: توزیع آماری دمای داخلی» و «۴-۱۲: ارتباط بین دما و پاداش» را قرار دهید.)

**سیستم ذخیره‌سازی انرژی (ESS):** مدیریت باتری در نسخه نهایی به شکل چشمگیری بهبود یافت. عامل یاد گرفته بود که سطح شارژ باتری را به‌صورت محافظه‌کارانه در یک بازه میانی (عمدتاً بین ۹۰ تا ۱۳۰ کیلووات‌ساعت) نگه دارد تا هم از تخلیه شدید جلوگیری کند و هم برای ذخیره انرژی خورشیدی ظرفیت کافی داشته باشد. تعداد چرخه‌های شارژ و دشارژ نیز به تعادل رسید که به افزایش طول عمر باتری کمک می‌کند. نمودار ارتباط پاداش و سطح شارژ نیز تأیید می‌کند که عامل از سطوح شارژ بسیار پایین یا بسیار بالا که منجر به جریمه می‌شوند، اجتناب کرده است.

(در این بخش می‌توانید نمودارهای مربوط به ESS از جمله «SOC Over Time»، «Action Distribution»، «SOC Distribution» و «Reward vs SOC» را قرار دهید.)

**پمپ آب:** عملکرد پمپ آب نیز بسیار پایدارتر شد. عامل یاد گرفت که سطح آب مخزن را همواره نزدیک به ظرفیت کامل نگه دارد تا از افت فشار و عملکرد اضطراری پمپ جلوگیری کند. اگرچه توزیع اقدامات همچنان تمایل به تصمیمات قاطع (روشن/خاموش کامل) را نشان می‌دهد، اما تثبیت سطح آب و ارتباط آن با پاداش بالا، بیانگر یادگیری یک سیاست کارآمد برای مدیریت این منبع است.

(در این بخش می‌توانید نمودارهای مربوط به پمپ آب از جمله «Tank Water Level Over Time»، «Tank Level Distribution»، «Action Distribution» و «Reward vs Tank Level» را قرار دهید.)

**شبکه برق و انرژی خورشیدی:** در نهایت، تعامل با شبکه و بهره‌برداری از انرژی خورشیدی نیز بهینه شد. اگرچه وابستگی به شبکه همچنان حدود ۷۳٪ بود، اما عامل یاد گرفت که خرید برق را به ساعات کم‌هزینه منتقل کند. این موضوع در نمودار هزینه شبکه که پس از مدتی حول یک میانگین پایین تثبیت می‌شود، کاملاً مشهود است. میزان فروش انرژی خورشیدی به شبکه نیز به ۲۷٪ رسید که نشان‌دهنده بهره‌برداری بهتر از منابع تجدیدپذیر است، هرچند هنوز ظرفیت برای بهبود و ذخیره‌سازی بیشتر انرژی مازاد وجود دارد.

(در این بخش می‌توانید نمودارهای مربوط به شبکه و هزینه از جمله «Buy vs Sell Energy»، «Grid Cost Over Time» و «Reward vs Grid Cost» را قرار دهید.)

### ۳-۴-۴ مقایسه شاخص‌های کلیدی با پژوهش‌های مرجع

برای سنجش اعتبار علمی مدل، نتایج آن با چندین پژوهش معتبر مقایسه شد. مدل پیشنهادی با کاهش ۲۵.۴٪ در هزینه انرژی، عملکردی رقابتی و حتی برتر از بسیاری از مدل‌های مشابه DRL ارائه داد. در شاخص آسایش حرارتی نیز با حفظ دمای مطلوب در ۹۹.۲٪ مواقع، پایداری فوق‌العاده‌ای از خود نشان داد. همچنین، نرخ بهره‌برداری از انرژی خورشیدی (ترکیب مصرف مستقیم و فروش) یکی از بالاترین ارقام گزارش‌شده در میان مقالات مورد بررسی بود. این نتایج نشان می‌دهد که مدل توسعه‌یافته نه تنها از نظر فنی موفق بوده، بلکه از منظر علمی نیز دستاوردهای قابل توجهی داشته و قابلیت تعمیم و پیاده‌سازی در سیستم‌های واقعی را داراست.

(در این بخش می‌توانید «جدول مقایسه شاخص‌های مدل با پژوهش‌های مرجع» را قرار دهید.)